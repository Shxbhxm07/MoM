from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import logging
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Llama 3 Meeting Intelligence Service", version="4.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

tokenizer = None
model = None

MAX_INPUT_TOKENS = 6000
CHUNK_SIZE_CHARS = 10000  # Larger chunks = fewer synthesis steps
MAX_NEW_TOKENS_PER_CHUNK = 2000
MAX_NEW_TOKENS_SYNTHESIS = 4000  # DOUBLED for synthesis to prevent cutoff

# =========================
# COMPREHENSIVE MOM PROMPT
# =========================
MEETING_ANALYSIS_SYSTEM_PROMPT = """
You are Docutalk - an AI that creates detailed meeting documentation.

Generate a comprehensive Minutes of Meeting document.

FORMAT (follow exactly):

================================================================================
MoM | [Meeting Name]
Date: [DD MMM YYYY]
Time: [Time or "Not specified"]
================================================================================

AGENDA
  â€¢ [Each agenda item with details]

________________________________________________________________________________

ATTENDEES

  Committee Members:
    - [Name] (SPEAKER_X): [Role]

  Staff:
    - [Name] (SPEAKER_X): [Role]

________________________________________________________________________________

SUMMARY (Key Discussion Points)

  [Write 6-8 comprehensive sentences covering the ENTIRE meeting from start
  to finish. Include: opening, main discussions, all decisions made, vote
  outcomes, next steps, and closing. Be thorough and complete.]

________________________________________________________________________________

SPEAKER-WISE NOTES (Who Said What)

  [Name] - [Role]
    â€¢ [Contribution 1 - detailed]
    â€¢ [Contribution 2 - include specifics]
    â€¢ [Continue for ALL contributions]
    â€¢ [Active speakers need 10-20 bullets minimum]

  [Next Speaker] - [Role]
    â€¢ [All contributions]

________________________________________________________________________________

DECISIONS TAKEN

  D1. [Decision with vote count and outcome]
  D2. [Next decision]
  [ALL votes documented]

________________________________________________________________________________

ACTION ITEMS

  [Task]
    Owner: [Name]
    Due Date: [Date]

________________________________________________________________________________

PURPOSE OF MEETING

  [3-4 sentences explaining purpose and outcomes]

================================================================================

Regards,
Generated by AngelBot.AI


CRITICAL RULES:
- Use â€¢ for bullets (NOT +, -, or *)
- Start with ================ (no preamble)
- Document EVERYTHING each speaker said
- Include ALL votes with counts
- Generate COMPLETE sections (don't stop early)
- Priority: Completeness > Brevity
""".strip()

# =========================
# SYNTHESIS PROMPT - EXTENDED FOR COMPLETENESS
# =========================
SYNTHESIS_SYSTEM_PROMPT = """
You are Docutalk - synthesizing partial meeting analyses into ONE complete document.

CRITICAL TASK: Combine all partial analyses into ONE comprehensive, complete
Minutes of Meeting. Your output must be COMPLETE - do not stop mid-section.

SYNTHESIS RULES:

1. MERGE SPEAKER CONTRIBUTIONS
   - Combine all bullets for each speaker
   - Remove duplicates but keep all unique points
   - Organize chronologically
   - Result: Each active speaker should have 10-20+ bullets

2. CONSOLIDATE DECISIONS
   - List every vote taken
   - Include vote counts
   - Remove duplicates
   - Keep chronological order

3. COMBINE ACTION ITEMS
   - Merge all action items
   - Remove duplicates
   - Keep all unique tasks

4. CREATE COMPREHENSIVE SUMMARY
   - Write 6-8 sentences covering ENTIRE meeting
   - Include beginning, middle, and end
   - Mention all major topics
   - State outcomes and next steps

5. ENSURE COMPLETENESS
   - Generate ALL sections fully
   - Don't stop mid-sentence or mid-section
   - Complete the document properly
   - End with footer

OUTPUT FORMAT: Start with ================ and generate complete MoM document.

PRIORITY: Generate the COMPLETE document. Don't truncate. Finish all sections
properly including Purpose, footer, and "Regards" sign-off.

BEGIN:
""".strip()

# =========================
# REQUEST MODELS
# =========================
class SummarizeRequest(BaseModel):
    text: str
    max_length: int = 4000  # INCREASED for final synthesis
    temperature: float = 0.01  # Ultra-low for consistency

class TranslateRequest(BaseModel):
    text: str
    source_lang: str = "en"
    target_lang: str = "hi"
    max_length: int = 2000
    temperature: float = 0.2

# =========================
# MODEL LOADING
# =========================
@app.on_event("startup")
async def load_model():
    global tokenizer, model
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"

        if torch.cuda.is_available():
            logger.info(f"GPU: {torch.cuda.get_device_name(0)}")

        model_path = "/app/Meta-Llama-3-8B-Instruct"
        logger.info(f"Loading model from: {model_path}")

        tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            local_files_only=True,
        )

        logger.info(f"âœ“ Model loaded on {device}")
        logger.info("ðŸš€ Optimized Chunking MoM service ready (v4.0)")

    except Exception as e:
        logger.error(f"Model load failed: {e}")
        raise

# =========================
# HEALTH
# =========================
@app.get("/")
async def root():
    return {
        "service": "Llama 3 Meeting Intelligence",
        "version": "4.0.0 - Optimized Chunking",
        "max_synthesis_tokens": MAX_NEW_TOKENS_SYNTHESIS
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy" if model else "loading",
        "model_loaded": model is not None,
        "gpu_available": torch.cuda.is_available()
    }

# =========================
# SMART CHUNKING
# =========================
def chunk_transcript_smartly(transcript: str) -> list:
    """Chunk at natural boundaries (speaker changes)"""
    lines = transcript.split('\n')
    chunks = []
    current_chunk = []
    current_length = 0
    
    for line in lines:
        line_length = len(line)
        
        # Check if adding line exceeds chunk size
        if current_length + line_length > CHUNK_SIZE_CHARS and current_chunk:
            chunks.append('\n'.join(current_chunk))
            current_chunk = [line]
            current_length = line_length
        else:
            current_chunk.append(line)
            current_length += line_length + 1
    
    if current_chunk:
        chunks.append('\n'.join(current_chunk))
    
    logger.info(f"[CHUNK] Created {len(chunks)} chunks")
    return chunks

# =========================
# GENERATION WITH EXTENDED TOKEN LIMIT
# =========================
def generate_with_prompt(
    system_prompt: str,
    user_message: str,
    max_length: int,
    temperature: float,
    is_synthesis: bool = False
) -> str:
    """Generate with special handling for synthesis"""
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message},
    ]

    prompt = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=MAX_INPUT_TOKENS,
    ).to(model.device)

    input_len = inputs.input_ids.shape[1]
    
    # Use higher token limit for synthesis
    actual_max_length = max_length if not is_synthesis else MAX_NEW_TOKENS_SYNTHESIS

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=actual_max_length,
            temperature=temperature if temperature > 0 else 0.01,
            do_sample=True,
            top_p=0.95,
            repetition_penalty=1.1,  # Lower to allow completing thoughts
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    generated_ids = outputs[0][input_len:]
    result = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()

    result = result.replace("<|eot_id|>", "").strip()
    if "assistant" in result.lower():
        result = result.split("assistant")[-1].strip()

    return result

# =========================
# CLEANUP
# =========================
def clean_mom_output(text: str) -> str:
    """Clean MoM output"""
    
    # Remove preambles
    preambles = [
        r'^here\s+is\s+the\s+(synthesized\s+)?minutes',
        r'^here\s+is\s+the\s+mom',
        r'^minutes\s+of\s+meeting[:\s]*\n',
        r'^i\s+have\s+(generated|created)',
        r'^based\s+on\s+(the\s+)?',
    ]
    
    for pattern in preambles:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)
    
    # Remove Markdown
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
    text = re.sub(r'\*([^*]+)\*', r'\1', text)
    text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)
    
    # Fix bullets
    text = re.sub(r'^\s*\+\s+', '  â€¢ ', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*-\s+([^S])', r'  â€¢ \1', text, flags=re.MULTILINE)  # Keep - for names
    
    # Clean blank lines
    text = re.sub(r'\n{4,}', '\n\n\n', text)
    
    # Ensure starts with separator
    if not text.strip().startswith('==='):
        text = '=' * 80 + '\n' + text
    
    return text.strip()

# =========================
# MAIN ENDPOINT WITH OPTIMIZED CHUNKING
# =========================
@app.post("/summarize")
async def summarize(request: SummarizeRequest):
    if not model or not tokenizer:
        raise HTTPException(503, "Model not loaded")

    try:
        text_len = len(request.text)
        estimated_tokens = text_len // 4
        
        logger.info(f"[MOM] Processing ({text_len} chars, ~{estimated_tokens} tokens)")

        # === SHORT MEETINGS: Single pass ===
        if estimated_tokens < 3500:  # Lowered threshold
            logger.info("[MOM] Single-pass generation")
            
            user_prompt = f"""Generate comprehensive Minutes of Meeting.

CRITICAL:
- Start with ================================================================================
- NO preamble
- Use â€¢ for bullets
- Complete ALL sections
- Don't stop early

Transcription:

{request.text}"""

            analysis = generate_with_prompt(
                MEETING_ANALYSIS_SYSTEM_PROMPT,
                user_prompt,
                3000,  # Generous for single pass
                request.temperature,
                is_synthesis=False
            )
            
            analysis = clean_mom_output(analysis)

        # === LONG MEETINGS: Chunking + Synthesis ===
        else:
            logger.info("[MOM] Chunking approach for long meeting")
            
            # Step 1: Chunk
            chunks = chunk_transcript_smartly(request.text)
            
            # Step 2: Analyze each chunk
            partial_analyses = []
            
            for i, chunk in enumerate(chunks):
                logger.info(f"[MOM] Analyzing chunk {i+1}/{len(chunks)} ({len(chunk)} chars)")
                
                chunk_prompt = f"""Analyze this meeting transcript section.

Extract complete information:
- All speaker contributions (every point made)
- All decisions (with vote counts)
- All action items (with owners and dates)
- Discussion details

Chunk {i+1}/{len(chunks)}:

{chunk}"""

                partial = generate_with_prompt(
                    MEETING_ANALYSIS_SYSTEM_PROMPT,
                    chunk_prompt,
                    MAX_NEW_TOKENS_PER_CHUNK,
                    request.temperature,
                    is_synthesis=False
                )
                
                partial_analyses.append(partial)
                logger.info(f"[MOM] Chunk {i+1} done ({len(partial)} chars)")
            
            # Step 3: Synthesize with EXTENDED token limit
            logger.info("[MOM] Synthesizing into complete MoM")
            logger.info(f"[MOM] Using {MAX_NEW_TOKENS_SYNTHESIS} tokens for synthesis")
            
            combined = "\n\n=== CHUNK BOUNDARY ===\n\n".join(partial_analyses)
            
            synthesis_prompt = f"""Synthesize these {len(chunks)} partial analyses into ONE complete MoM.

CRITICAL REQUIREMENTS:
1. Start with ================================================================================
2. Merge ALL speaker contributions (keep every bullet)
3. Combine ALL decisions with vote counts
4. List ALL action items
5. Write complete summary (6-8 sentences covering entire meeting)
6. FINISH the document completely (don't stop early)
7. Include Purpose section
8. Include footer with "Regards"

Partial analyses:

{combined}

Generate COMPLETE Minutes of Meeting:"""

            analysis = generate_with_prompt(
                SYNTHESIS_SYSTEM_PROMPT,
                synthesis_prompt,
                MAX_NEW_TOKENS_SYNTHESIS,  # Use extended limit
                request.temperature,
                is_synthesis=True  # Flag for special handling
            )
            
            analysis = clean_mom_output(analysis)
            
            # Validate synthesis completed
            if not analysis.endswith("Regards,\nGenerated by AngelBot.AI"):
                logger.warning("[MOM] Synthesis may be incomplete - missing footer")
                # Append footer if missing
                if "Regards" not in analysis[-200:]:
                    analysis += "\n\n" + "=" * 80 + "\n\nRegards,\nGenerated by AngelBot.AI\n"
        
        logger.info(f"[MOM] âœ“ Complete ({len(analysis)} chars)")

        return {
            "analysis": analysis,
            "original_length": text_len,
            "analysis_length": len(analysis),
            "method": "single_pass" if estimated_tokens < 3500 else "chunking_synthesis",
            "chunks_used": 1 if estimated_tokens < 3500 else len(chunks)
        }

    except Exception as e:
        logger.error(f"[MOM] Failed: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(500, str(e))

# =========================
# TRANSLATION
# =========================
@app.post("/translate")
async def translate(request: TranslateRequest):
    if not model or not tokenizer:
        raise HTTPException(503, "Model not loaded")

    try:
        logger.info(f"[TRANSLATE] {request.source_lang} â†’ {request.target_lang}")

        instruction = f"Translate from {request.source_lang} to {request.target_lang}. Preserve timestamps and speaker labels. Output only translation."
        
        translated = generate_with_prompt(
            "You are a translator. Output only the translation.",
            f"{instruction}\n\n{request.text}",
            request.max_length,
            request.temperature,
            is_synthesis=False
        )

        # Remove preambles
        for phrase in ["here is", "translation:"]:
            if translated.lower().startswith(phrase):
                lines = translated.split('\n', 1)
                if len(lines) > 1:
                    translated = lines[1].strip()
                break

        logger.info(f"[TRANSLATE] âœ“ Done ({len(translated)} chars)")

        return {
            "translated_text": translated,
            "source_lang": request.source_lang,
            "target_lang": request.target_lang
        }

    except Exception as e:
        logger.error(f"[TRANSLATE] Failed: {e}")
        raise HTTPException(500, str(e))