# from fastapi import FastAPI, HTTPException
# from fastapi.middleware.cors import CORSMiddleware
# from pydantic import BaseModel
# from transformers import AutoTokenizer, AutoModelForCausalLM
# import torch
# import logging
# import re

# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# app = FastAPI(title="Llama 3 Meeting Intelligence Service", version="4.0.0")

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# tokenizer = None
# model = None

# MAX_INPUT_TOKENS = 6000
# CHUNK_SIZE_CHARS = 10000  # Larger chunks = fewer synthesis steps
# MAX_NEW_TOKENS_PER_CHUNK = 2000
# MAX_NEW_TOKENS_SYNTHESIS = 4000  # DOUBLED for synthesis to prevent cutoff

# # =========================
# # COMPREHENSIVE MOM PROMPT
# # =========================
# MEETING_ANALYSIS_SYSTEM_PROMPT = """
# You are Docutalk - an AI that creates detailed meeting documentation.

# Generate a comprehensive Minutes of Meeting document.

# FORMAT (follow exactly):

# ================================================================================
# MoM | [Meeting Name]
# Date: [DD MMM YYYY]
# Time: [Time or "Not specified"]
# ================================================================================

# AGENDA
#   â€¢ [Each agenda item with details]

# ________________________________________________________________________________

# ATTENDEES

#   Committee Members:
#     - [Name] (SPEAKER_X): [Role]

#   Staff:
#     - [Name] (SPEAKER_X): [Role]

# ________________________________________________________________________________

# SUMMARY (Key Discussion Points)

#   [Write 6-8 comprehensive sentences covering the ENTIRE meeting from start
#   to finish. Include: opening, main discussions, all decisions made, vote
#   outcomes, next steps, and closing. Be thorough and complete.]

# ________________________________________________________________________________

# SPEAKER-WISE NOTES (Who Said What)

#   [Name] - [Role]
#     â€¢ [Contribution 1 - detailed]
#     â€¢ [Contribution 2 - include specifics]
#     â€¢ [Continue for ALL contributions]
#     â€¢ [Active speakers need 10-20 bullets minimum]

#   [Next Speaker] - [Role]
#     â€¢ [All contributions]

# ________________________________________________________________________________

# DECISIONS TAKEN

#   D1. [Decision with vote count and outcome]
#   D2. [Next decision]
#   [ALL votes documented]

# ________________________________________________________________________________

# ACTION ITEMS

#   [Task]
#     Owner: [Name]
#     Due Date: [Date]

# ________________________________________________________________________________

# PURPOSE OF MEETING

#   [3-4 sentences explaining purpose and outcomes]

# ================================================================================

# Regards,
# Generated by AngelBot.AI


# CRITICAL RULES:
# - Use â€¢ for bullets (NOT +, -, or *)
# - Start with ================ (no preamble)
# - Document EVERYTHING each speaker said
# - Include ALL votes with counts
# - Generate COMPLETE sections (don't stop early)
# - Priority: Completeness > Brevity
# """.strip()

# # =========================
# # SYNTHESIS PROMPT - EXTENDED FOR COMPLETENESS
# # =========================
# SYNTHESIS_SYSTEM_PROMPT = """
# You are Docutalk - synthesizing partial meeting analyses into ONE complete document.

# CRITICAL TASK: Combine all partial analyses into ONE comprehensive, complete
# Minutes of Meeting. Your output must be COMPLETE - do not stop mid-section.

# SYNTHESIS RULES:

# 1. MERGE SPEAKER CONTRIBUTIONS
#    - Combine all bullets for each speaker
#    - Remove duplicates but keep all unique points
#    - Organize chronologically
#    - Result: Each active speaker should have 10-20+ bullets

# 2. CONSOLIDATE DECISIONS
#    - List every vote taken
#    - Include vote counts
#    - Remove duplicates
#    - Keep chronological order

# 3. COMBINE ACTION ITEMS
#    - Merge all action items
#    - Remove duplicates
#    - Keep all unique tasks

# 4. CREATE COMPREHENSIVE SUMMARY
#    - Write 6-8 sentences covering ENTIRE meeting
#    - Include beginning, middle, and end
#    - Mention all major topics
#    - State outcomes and next steps

# 5. ENSURE COMPLETENESS
#    - Generate ALL sections fully
#    - Don't stop mid-sentence or mid-section
#    - Complete the document properly
#    - End with footer

# OUTPUT FORMAT: Start with ================ and generate complete MoM document.

# PRIORITY: Generate the COMPLETE document. Don't truncate. Finish all sections
# properly including Purpose, footer, and "Regards" sign-off.

# BEGIN:
# """.strip()

# # =========================
# # REQUEST MODELS
# # =========================
# class SummarizeRequest(BaseModel):
#     text: str
#     max_length: int = 4000  # INCREASED for final synthesis
#     temperature: float = 0.01  # Ultra-low for consistency

# class TranslateRequest(BaseModel):
#     text: str
#     source_lang: str = "en"
#     target_lang: str = "hi"
#     max_length: int = 2000
#     temperature: float = 0.2

# # =========================
# # MODEL LOADING
# # =========================
# @app.on_event("startup")
# async def load_model():
#     global tokenizer, model
#     try:
#         device = "cuda" if torch.cuda.is_available() else "cpu"

#         if torch.cuda.is_available():
#             logger.info(f"GPU: {torch.cuda.get_device_name(0)}")

#         model_path = "/app/Meta-Llama-3-8B-Instruct"
#         logger.info(f"Loading model from: {model_path}")

#         tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
#         model = AutoModelForCausalLM.from_pretrained(
#             model_path,
#             torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
#             device_map="auto",
#             local_files_only=True,
#         )

#         logger.info(f"âœ“ Model loaded on {device}")
#         logger.info("ðŸš€ Optimized Chunking MoM service ready (v4.0)")

#     except Exception as e:
#         logger.error(f"Model load failed: {e}")
#         raise

# # =========================
# # HEALTH
# # =========================
# @app.get("/")
# async def root():
#     return {
#         "service": "Llama 3 Meeting Intelligence",
#         "version": "4.0.0 - Optimized Chunking",
#         "max_synthesis_tokens": MAX_NEW_TOKENS_SYNTHESIS
#     }

# @app.get("/health")
# async def health():
#     return {
#         "status": "healthy" if model else "loading",
#         "model_loaded": model is not None,
#         "gpu_available": torch.cuda.is_available()
#     }

# # =========================
# # SMART CHUNKING
# # =========================
# def chunk_transcript_smartly(transcript: str) -> list:
#     """Chunk at natural boundaries (speaker changes)"""
#     lines = transcript.split('\n')
#     chunks = []
#     current_chunk = []
#     current_length = 0
    
#     for line in lines:
#         line_length = len(line)
        
#         # Check if adding line exceeds chunk size
#         if current_length + line_length > CHUNK_SIZE_CHARS and current_chunk:
#             chunks.append('\n'.join(current_chunk))
#             current_chunk = [line]
#             current_length = line_length
#         else:
#             current_chunk.append(line)
#             current_length += line_length + 1
    
#     if current_chunk:
#         chunks.append('\n'.join(current_chunk))
    
#     logger.info(f"[CHUNK] Created {len(chunks)} chunks")
#     return chunks

# # =========================
# # GENERATION WITH EXTENDED TOKEN LIMIT
# # =========================
# def generate_with_prompt(
#     system_prompt: str,
#     user_message: str,
#     max_length: int,
#     temperature: float,
#     is_synthesis: bool = False
# ) -> str:
#     """Generate with special handling for synthesis"""
#     messages = [
#         {"role": "system", "content": system_prompt},
#         {"role": "user", "content": user_message},
#     ]

#     prompt = tokenizer.apply_chat_template(
#         messages, tokenize=False, add_generation_prompt=True
#     )

#     inputs = tokenizer(
#         prompt,
#         return_tensors="pt",
#         truncation=True,
#         max_length=MAX_INPUT_TOKENS,
#     ).to(model.device)

#     input_len = inputs.input_ids.shape[1]
    
#     # Use higher token limit for synthesis
#     actual_max_length = max_length if not is_synthesis else MAX_NEW_TOKENS_SYNTHESIS

#     with torch.no_grad():
#         outputs = model.generate(
#             **inputs,
#             max_new_tokens=actual_max_length,
#             temperature=temperature if temperature > 0 else 0.01,
#             do_sample=True,
#             top_p=0.95,
#             repetition_penalty=1.1,  # Lower to allow completing thoughts
#             pad_token_id=tokenizer.eos_token_id,
#             eos_token_id=tokenizer.eos_token_id,
#         )

#     generated_ids = outputs[0][input_len:]
#     result = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()

#     result = result.replace("<|eot_id|>", "").strip()
#     if "assistant" in result.lower():
#         result = result.split("assistant")[-1].strip()

#     return result

# # =========================
# # CLEANUP
# # =========================
# def clean_mom_output(text: str) -> str:
#     """Clean MoM output"""
    
#     # Remove preambles
#     preambles = [
#         r'^here\s+is\s+the\s+(synthesized\s+)?minutes',
#         r'^here\s+is\s+the\s+mom',
#         r'^minutes\s+of\s+meeting[:\s]*\n',
#         r'^i\s+have\s+(generated|created)',
#         r'^based\s+on\s+(the\s+)?',
#     ]
    
#     for pattern in preambles:
#         text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)
    
#     # Remove Markdown
#     text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
#     text = re.sub(r'\*([^*]+)\*', r'\1', text)
#     text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)
    
#     # Fix bullets
#     text = re.sub(r'^\s*\+\s+', '  â€¢ ', text, flags=re.MULTILINE)
#     text = re.sub(r'^\s*-\s+([^S])', r'  â€¢ \1', text, flags=re.MULTILINE)  # Keep - for names
    
#     # Clean blank lines
#     text = re.sub(r'\n{4,}', '\n\n\n', text)
    
#     # Ensure starts with separator
#     if not text.strip().startswith('==='):
#         text = '=' * 80 + '\n' + text
    
#     return text.strip()

# # =========================
# # MAIN ENDPOINT WITH OPTIMIZED CHUNKING
# # =========================
# @app.post("/summarize")
# async def summarize(request: SummarizeRequest):
#     if not model or not tokenizer:
#         raise HTTPException(503, "Model not loaded")

#     try:
#         text_len = len(request.text)
#         estimated_tokens = text_len // 4
        
#         logger.info(f"[MOM] Processing ({text_len} chars, ~{estimated_tokens} tokens)")

#         # === SHORT MEETINGS: Single pass ===
#         if estimated_tokens < 3500:  # Lowered threshold
#             logger.info("[MOM] Single-pass generation")
            
#             user_prompt = f"""Generate comprehensive Minutes of Meeting.

# CRITICAL:
# - Start with ================================================================================
# - NO preamble
# - Use â€¢ for bullets
# - Complete ALL sections
# - Don't stop early

# Transcription:

# {request.text}"""

#             analysis = generate_with_prompt(
#                 MEETING_ANALYSIS_SYSTEM_PROMPT,
#                 user_prompt,
#                 3000,  # Generous for single pass
#                 request.temperature,
#                 is_synthesis=False
#             )
            
#             analysis = clean_mom_output(analysis)

#         # === LONG MEETINGS: Chunking + Synthesis ===
#         else:
#             logger.info("[MOM] Chunking approach for long meeting")
            
#             # Step 1: Chunk
#             chunks = chunk_transcript_smartly(request.text)
            
#             # Step 2: Analyze each chunk
#             partial_analyses = []
            
#             for i, chunk in enumerate(chunks):
#                 logger.info(f"[MOM] Analyzing chunk {i+1}/{len(chunks)} ({len(chunk)} chars)")
                
#                 chunk_prompt = f"""Analyze this meeting transcript section.

# Extract complete information:
# - All speaker contributions (every point made)
# - All decisions (with vote counts)
# - All action items (with owners and dates)
# - Discussion details

# Chunk {i+1}/{len(chunks)}:

# {chunk}"""

#                 partial = generate_with_prompt(
#                     MEETING_ANALYSIS_SYSTEM_PROMPT,
#                     chunk_prompt,
#                     MAX_NEW_TOKENS_PER_CHUNK,
#                     request.temperature,
#                     is_synthesis=False
#                 )
                
#                 partial_analyses.append(partial)
#                 logger.info(f"[MOM] Chunk {i+1} done ({len(partial)} chars)")
            
#             # Step 3: Synthesize with EXTENDED token limit
#             logger.info("[MOM] Synthesizing into complete MoM")
#             logger.info(f"[MOM] Using {MAX_NEW_TOKENS_SYNTHESIS} tokens for synthesis")
            
#             combined = "\n\n=== CHUNK BOUNDARY ===\n\n".join(partial_analyses)
            
#             synthesis_prompt = f"""Synthesize these {len(chunks)} partial analyses into ONE complete MoM.

# CRITICAL REQUIREMENTS:
# 1. Start with ================================================================================
# 2. Merge ALL speaker contributions (keep every bullet)
# 3. Combine ALL decisions with vote counts
# 4. List ALL action items
# 5. Write complete summary (6-8 sentences covering entire meeting)
# 6. FINISH the document completely (don't stop early)
# 7. Include Purpose section
# 8. Include footer with "Regards"

# Partial analyses:

# {combined}

# Generate COMPLETE Minutes of Meeting:"""

#             analysis = generate_with_prompt(
#                 SYNTHESIS_SYSTEM_PROMPT,
#                 synthesis_prompt,
#                 MAX_NEW_TOKENS_SYNTHESIS,  # Use extended limit
#                 request.temperature,
#                 is_synthesis=True  # Flag for special handling
#             )
            
#             analysis = clean_mom_output(analysis)
            
#             # Validate synthesis completed
#             if not analysis.endswith("Regards,\nGenerated by AngelBot.AI"):
#                 logger.warning("[MOM] Synthesis may be incomplete - missing footer")
#                 # Append footer if missing
#                 if "Regards" not in analysis[-200:]:
#                     analysis += "\n\n" + "=" * 80 + "\n\nRegards,\nGenerated by AngelBot.AI\n"
        
#         logger.info(f"[MOM] âœ“ Complete ({len(analysis)} chars)")

#         return {
#             "analysis": analysis,
#             "original_length": text_len,
#             "analysis_length": len(analysis),
#             "method": "single_pass" if estimated_tokens < 3500 else "chunking_synthesis",
#             "chunks_used": 1 if estimated_tokens < 3500 else len(chunks)
#         }

#     except Exception as e:
#         logger.error(f"[MOM] Failed: {e}")
#         import traceback
#         traceback.print_exc()
#         raise HTTPException(500, str(e))

# # =========================
# # TRANSLATION
# # =========================
# @app.post("/translate")
# async def translate(request: TranslateRequest):
#     if not model or not tokenizer:
#         raise HTTPException(503, "Model not loaded")

#     try:
#         logger.info(f"[TRANSLATE] {request.source_lang} â†’ {request.target_lang}")

#         instruction = f"Translate from {request.source_lang} to {request.target_lang}. Preserve timestamps and speaker labels. Output only translation."
        
#         translated = generate_with_prompt(
#             "You are a translator. Output only the translation.",
#             f"{instruction}\n\n{request.text}",
#             request.max_length,
#             request.temperature,
#             is_synthesis=False
#         )

#         # Remove preambles
#         for phrase in ["here is", "translation:"]:
#             if translated.lower().startswith(phrase):
#                 lines = translated.split('\n', 1)
#                 if len(lines) > 1:
#                     translated = lines[1].strip()
#                 break

#         logger.info(f"[TRANSLATE] âœ“ Done ({len(translated)} chars)")

#         return {
#             "translated_text": translated,
#             "source_lang": request.source_lang,
#             "target_lang": request.target_lang
#         }

#     except Exception as e:
#         logger.error(f"[TRANSLATE] Failed: {e}")
#         raise HTTPException(500, str(e))

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import logging
import re
from typing import Optional, List, Tuple

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Llama 3 Meeting Intelligence Service", version="5.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

tokenizer = None
model = None

MAX_INPUT_TOKENS = 6000
CHUNK_SIZE_CHARS = 10000
MAX_NEW_TOKENS_PER_CHUNK = 2000
MAX_NEW_TOKENS_SYNTHESIS = 4000
MAX_TRANSLATION_TOKENS = 2000

# Translation settings
TRANSLATION_LINE_BY_LINE = True  # Process each line separately
USE_PIVOT_TRANSLATION = True     # Use English as intermediate for non-EN pairs
TRANSLATION_RETRIES = 2          # Retry failed translations

# =========================
# SUPPORTED LANGUAGES
# =========================
SUPPORTED_LANGUAGES = {
    "en": {
        "code": "en",
        "name": "English",
        "native_name": "English",
        "direction": "ltr",
        "script": "Latin"
    },
    "hi": {
        "code": "hi",
        "name": "Hindi",
        "native_name": "à¤¹à¤¿à¤¨à¥à¤¦à¥€",
        "direction": "ltr",
        "script": "Devanagari"
    },
    "he": {
        "code": "he",
        "name": "Hebrew",
        "native_name": "×¢×‘×¨×™×ª",
        "direction": "rtl",
        "script": "Hebrew"
    },
    "ar": {
        "code": "ar",
        "name": "Arabic",
        "native_name": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
        "direction": "rtl",
        "script": "Arabic"
    },
    "ru": {
        "code": "ru",
        "name": "Russian",
        "native_name": "Ð ÑƒÑÑÐºÐ¸Ð¹",
        "direction": "ltr",
        "script": "Cyrillic"
    }
}

# =========================
# IMPROVED TRANSLATION PROMPTS
# =========================

def get_translation_system_prompt(source_lang: str, target_lang: str) -> str:
    """Generate precise translation system prompt"""
    source_info = SUPPORTED_LANGUAGES.get(source_lang, {})
    target_info = SUPPORTED_LANGUAGES.get(target_lang, {})
    
    source_name = source_info.get('name', source_lang)
    target_name = target_info.get('name', target_lang)
    target_native = target_info.get('native_name', target_name)
    target_script = target_info.get('script', 'appropriate script')
    
    return f"""You are an expert {source_name} to {target_name} translator.

CRITICAL TRANSLATION RULES:
1. Translate the EXACT meaning - do not add, remove, or change content
2. Preserve the speaker's intent, emotion, and tone
3. Use natural {target_name} ({target_native}) expressions
4. Write in {target_script} script
5. Keep questions as questions, statements as statements
6. Preserve exclamations and emotional expressions
7. Do NOT reverse or swap meanings
8. Do NOT add explanations or notes
9. Output ONLY the translation

ACCURACY IS CRITICAL. Translate faithfully without any modifications to meaning."""


def get_single_line_translation_prompt(
    text: str, 
    source_lang: str, 
    target_lang: str,
    context: Optional[str] = None
) -> str:
    """Generate prompt for translating a single line"""
    source_info = SUPPORTED_LANGUAGES.get(source_lang, {})
    target_info = SUPPORTED_LANGUAGES.get(target_lang, {})
    
    source_name = source_info.get('name', source_lang)
    target_name = target_info.get('name', target_lang)
    
    context_hint = ""
    if context:
        context_hint = f"\nContext (previous line): {context}\n"
    
    return f"""Translate this {source_name} text to {target_name}.
{context_hint}
RULES:
- Translate EXACTLY what is said
- Keep the same meaning, tone, and emotion  
- Do NOT add or remove anything
- Do NOT change questions to statements or vice versa
- Output ONLY the translation, nothing else

{source_name}: {text}

{target_name}:"""


# =========================
# REQUEST MODELS
# =========================
class SummarizeRequest(BaseModel):
    text: str
    max_length: int = 4000
    temperature: float = 0.01

class TranslateRequest(BaseModel):
    text: str
    source_lang: str = "en"
    target_lang: str = "hi"
    max_length: int = 3000
    temperature: float = 0.1  # Lower temperature for more accuracy
    use_pivot: Optional[bool] = None  # Auto-decide if not specified

# =========================
# COMPREHENSIVE MOM PROMPT
# =========================
MEETING_ANALYSIS_SYSTEM_PROMPT = """
You are Docutalk - an AI that creates detailed meeting documentation.

Generate a comprehensive Minutes of Meeting document.

FORMAT (follow exactly):

================================================================================
MoM | [Meeting Name]
Date: [DD MMM YYYY]
Time: [Time or "Not specified"]
================================================================================

AGENDA
  â€¢ [Each agenda item with details]

________________________________________________________________________________

ATTENDEES

  Committee Members:
    - [Name] (SPEAKER_X): [Role]

  Staff:
    - [Name] (SPEAKER_X): [Role]

________________________________________________________________________________

SUMMARY (Key Discussion Points)

  [Write 6-8 comprehensive sentences covering the ENTIRE meeting from start
  to finish. Include: opening, main discussions, all decisions made, vote
  outcomes, next steps, and closing. Be thorough and complete.]

________________________________________________________________________________

SPEAKER-WISE NOTES (Who Said What)

  [Name] - [Role]
    â€¢ [Contribution 1 - detailed]
    â€¢ [Contribution 2 - include specifics]
    â€¢ [Continue for ALL contributions]
    â€¢ [Active speakers need 10-20 bullets minimum]

  [Next Speaker] - [Role]
    â€¢ [All contributions]

________________________________________________________________________________

DECISIONS TAKEN

  D1. [Decision with vote count and outcome]
  D2. [Next decision]
  [ALL votes documented]

________________________________________________________________________________

ACTION ITEMS

  [Task]
    Owner: [Name]
    Due Date: [Date]

________________________________________________________________________________

PURPOSE OF MEETING

  [3-4 sentences explaining purpose and outcomes]

================================================================================

Regards,
Generated by AngelBot.AI


CRITICAL RULES:
- Use â€¢ for bullets (NOT +, -, or *)
- Start with ================ (no preamble)
- Document EVERYTHING each speaker said
- Include ALL votes with counts
- Generate COMPLETE sections (don't stop early)
- Priority: Completeness > Brevity
""".strip()

# =========================
# SYNTHESIS PROMPT
# =========================
SYNTHESIS_SYSTEM_PROMPT = """
You are Docutalk - synthesizing partial meeting analyses into ONE complete document.

CRITICAL TASK: Combine all partial analyses into ONE comprehensive, complete
Minutes of Meeting. Your output must be COMPLETE - do not stop mid-section.

SYNTHESIS RULES:

1. MERGE SPEAKER CONTRIBUTIONS
   - Combine all bullets for each speaker
   - Remove duplicates but keep all unique points
   - Organize chronologically
   - Result: Each active speaker should have 10-20+ bullets

2. CONSOLIDATE DECISIONS
   - List every vote taken
   - Include vote counts
   - Remove duplicates
   - Keep chronological order

3. COMBINE ACTION ITEMS
   - Merge all action items
   - Remove duplicates
   - Keep all unique tasks

4. CREATE COMPREHENSIVE SUMMARY
   - Write 6-8 sentences covering ENTIRE meeting
   - Include beginning, middle, and end
   - Mention all major topics
   - State outcomes and next steps

5. ENSURE COMPLETENESS
   - Generate ALL sections fully
   - Don't stop mid-sentence or mid-section
   - Complete the document properly
   - End with footer

OUTPUT FORMAT: Start with ================ and generate complete MoM document.

BEGIN:
""".strip()

# =========================
# MODEL LOADING
# =========================
@app.on_event("startup")
async def load_model():
    global tokenizer, model
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"

        if torch.cuda.is_available():
            logger.info(f"GPU: {torch.cuda.get_device_name(0)}")

        model_path = "/app/Meta-Llama-3-8B-Instruct"
        logger.info(f"Loading model from: {model_path}")

        tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            local_files_only=True,
        )

        logger.info(f"âœ“ Model loaded on {device}")
        logger.info(f"ðŸŒ Supported languages: {list(SUPPORTED_LANGUAGES.keys())}")
        logger.info(f"ðŸ”„ Pivot translation enabled: {USE_PIVOT_TRANSLATION}")
        logger.info("ðŸš€ High-Accuracy Translation Service ready (v5.0)")

    except Exception as e:
        logger.error(f"Model load failed: {e}")
        raise

# =========================
# HEALTH & INFO ENDPOINTS
# =========================
@app.get("/")
async def root():
    return {
        "service": "Llama 3 Meeting Intelligence",
        "version": "5.0.0 - High-Accuracy Translation",
        "features": {
            "summarization": True,
            "translation": True,
            "pivot_translation": USE_PIVOT_TRANSLATION,
            "line_by_line": TRANSLATION_LINE_BY_LINE,
            "languages": list(SUPPORTED_LANGUAGES.keys())
        }
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy" if model else "loading",
        "model_loaded": model is not None,
        "gpu_available": torch.cuda.is_available(),
        "supported_languages": list(SUPPORTED_LANGUAGES.keys())
    }

@app.get("/languages")
async def get_languages():
    """Get all supported languages with details"""
    return {
        "languages": SUPPORTED_LANGUAGES,
        "pivot_language": "en",
        "note": "Non-English pairs use English as pivot for better accuracy"
    }

# =========================
# GENERATION HELPER
# =========================
def generate_with_prompt(
    system_prompt: str,
    user_message: str,
    max_length: int,
    temperature: float,
    is_synthesis: bool = False
) -> str:
    """Generate with LLM"""
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message},
    ]

    prompt = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=MAX_INPUT_TOKENS,
    ).to(model.device)

    input_len = inputs.input_ids.shape[1]
    actual_max_length = max_length if not is_synthesis else MAX_NEW_TOKENS_SYNTHESIS

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=actual_max_length,
            temperature=max(temperature, 0.01),
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.15,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )

    generated_ids = outputs[0][input_len:]
    result = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    result = result.replace("<|eot_id|>", "").strip()
    
    if "assistant" in result.lower()[:50]:
        result = result.split("assistant", 1)[-1].strip()

    return result

# =========================
# TRANSCRIPT LINE PARSER
# =========================
def parse_transcript_line(line: str) -> Tuple[Optional[str], Optional[str], str]:
    """
    Parse a transcript line into components
    Returns: (timestamp, speaker, text)
    """
    line = line.strip()
    
    if not line:
        return None, None, ""
    
    timestamp = None
    speaker = None
    text = line
    
    # Extract timestamp [HH:MM:SS]
    timestamp_match = re.match(r'^\[(\d{1,2}:\d{2}(?::\d{2})?)\]\s*', line)
    if timestamp_match:
        timestamp = timestamp_match.group(1)
        text = line[timestamp_match.end():]
    
    # Extract speaker SPEAKER_X:
    speaker_match = re.match(r'^(SPEAKER_\d+):\s*', text)
    if speaker_match:
        speaker = speaker_match.group(1)
        text = text[speaker_match.end():]
    
    return timestamp, speaker, text.strip()


def reconstruct_line(timestamp: Optional[str], speaker: Optional[str], text: str) -> str:
    """Reconstruct a transcript line from components"""
    parts = []
    
    if timestamp:
        parts.append(f"[{timestamp}]")
    
    if speaker:
        parts.append(f"{speaker}:")
    
    parts.append(text)
    
    return " ".join(parts)

# =========================
# SINGLE LINE TRANSLATION
# =========================
def translate_single_text(
    text: str,
    source_lang: str,
    target_lang: str,
    temperature: float = 0.1,
    context: Optional[str] = None
) -> str:
    """Translate a single piece of text with high accuracy"""
    
    if not text or len(text.strip()) < 2:
        return text
    
    system_prompt = get_translation_system_prompt(source_lang, target_lang)
    user_prompt = get_single_line_translation_prompt(text, source_lang, target_lang, context)
    
    # Calculate appropriate max tokens (roughly 2x input for safety)
    estimated_tokens = max(len(text) // 2, 100)
    max_tokens = min(estimated_tokens * 2, 500)
    
    translated = generate_with_prompt(
        system_prompt,
        user_prompt,
        max_tokens,
        temperature,
        is_synthesis=False
    )
    
    # Clean output
    translated = clean_translation_output(translated)
    
    return translated


def translate_via_pivot(
    text: str,
    source_lang: str,
    target_lang: str,
    temperature: float = 0.1,
    context: Optional[str] = None
) -> str:
    """
    Translate using English as pivot language
    source_lang â†’ English â†’ target_lang
    """
    logger.info(f"[PIVOT] {source_lang} â†’ en â†’ {target_lang}")
    
    # Step 1: Translate to English
    if source_lang != "en":
        english_text = translate_single_text(
            text, 
            source_lang, 
            "en", 
            temperature,
            context
        )
        logger.debug(f"[PIVOT] Step 1 (â†’EN): {english_text[:50]}...")
    else:
        english_text = text
    
    # Step 2: Translate from English to target
    if target_lang != "en":
        final_text = translate_single_text(
            english_text,
            "en",
            target_lang,
            temperature,
            None  # Context less useful for second hop
        )
        logger.debug(f"[PIVOT] Step 2 (ENâ†’): {final_text[:50]}...")
    else:
        final_text = english_text
    
    return final_text

# =========================
# CLEAN TRANSLATION OUTPUT
# =========================
def clean_translation_output(text: str) -> str:
    """Clean translation output - remove preambles and artifacts"""
    
    if not text:
        return ""
    
    # Remove common preambles
    preambles = [
        r'^here\s+is\s+the\s+translation[:\s]*',
        r'^translation[:\s]*',
        r'^translated\s+text[:\s]*',
        r'^the\s+translation\s+is[:\s]*',
        r'^output[:\s]*',
        r'^result[:\s]*',
        r'^\[translation\][:\s]*',
        r'^in\s+(hindi|arabic|hebrew|russian|english)[:\s]*',
        r'^(hindi|arabic|hebrew|russian|english)[:\s]*',
    ]
    
    for pattern in preambles:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # Remove trailing notes
    text = re.sub(r'\n+note:.*$', '', text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r'\n+\(note:.*$', '', text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r'\n+translation note:.*$', '', text, flags=re.IGNORECASE | re.DOTALL)
    
    # Remove quotes that might wrap the translation
    text = text.strip('"\'')
    
    # Clean whitespace
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = text.strip()
    
    return text

# =========================
# MAIN TRANSLATION FUNCTION
# =========================
def translate_transcript(
    transcript: str,
    source_lang: str,
    target_lang: str,
    temperature: float = 0.1,
    use_pivot: bool = True
) -> str:
    """
    Translate a full transcript line-by-line with speaker preservation
    """
    lines = transcript.split('\n')
    translated_lines = []
    previous_text = None
    
    total_lines = len([l for l in lines if l.strip()])
    processed = 0
    
    logger.info(f"[TRANSLATE] Processing {total_lines} lines")
    
    for line in lines:
        if not line.strip():
            translated_lines.append("")
            continue
        
        # Parse the line
        timestamp, speaker, text = parse_transcript_line(line)
        
        # Skip if no actual text to translate
        if not text or len(text.strip()) < 2:
            translated_lines.append(line)
            continue
        
        processed += 1
        
        if processed % 10 == 0:
            logger.info(f"[TRANSLATE] Progress: {processed}/{total_lines}")
        
        # Decide translation method
        needs_pivot = (
            use_pivot and 
            source_lang != "en" and 
            target_lang != "en"
        )
        
        try:
            if needs_pivot:
                translated_text = translate_via_pivot(
                    text,
                    source_lang,
                    target_lang,
                    temperature,
                    previous_text
                )
            else:
                translated_text = translate_single_text(
                    text,
                    source_lang,
                    target_lang,
                    temperature,
                    previous_text
                )
            
            # Validate translation
            if not translated_text or len(translated_text.strip()) < 2:
                logger.warning(f"[TRANSLATE] Empty result, keeping original: {text[:30]}...")
                translated_text = text
            
            # Check for obvious errors (translation same as input for different scripts)
            if translated_text == text and source_lang != target_lang:
                # Retry once
                logger.warning(f"[TRANSLATE] Retrying unchanged translation...")
                if needs_pivot:
                    translated_text = translate_via_pivot(text, source_lang, target_lang, temperature * 1.5, previous_text)
                else:
                    translated_text = translate_single_text(text, source_lang, target_lang, temperature * 1.5, previous_text)
        
        except Exception as e:
            logger.error(f"[TRANSLATE] Line failed: {e}")
            translated_text = text  # Keep original on error
        
        # Reconstruct line with translated text
        translated_line = reconstruct_line(timestamp, speaker, translated_text)
        translated_lines.append(translated_line)
        
        # Store for context
        previous_text = text[:100]  # Keep first 100 chars as context
    
    logger.info(f"[TRANSLATE] âœ“ Completed {processed} lines")
    
    return '\n'.join(translated_lines)

# =========================
# SMART CHUNKING
# =========================
def chunk_transcript_smartly(transcript: str) -> list:
    """Chunk at natural boundaries"""
    lines = transcript.split('\n')
    chunks = []
    current_chunk = []
    current_length = 0
    
    for line in lines:
        line_length = len(line)
        
        if current_length + line_length > CHUNK_SIZE_CHARS and current_chunk:
            chunks.append('\n'.join(current_chunk))
            current_chunk = [line]
            current_length = line_length
        else:
            current_chunk.append(line)
            current_length += line_length + 1
    
    if current_chunk:
        chunks.append('\n'.join(current_chunk))
    
    return chunks

# =========================
# CLEANUP FOR MOM
# =========================
def clean_mom_output(text: str) -> str:
    """Clean MoM output"""
    preambles = [
        r'^here\s+is\s+the\s+(synthesized\s+)?minutes',
        r'^here\s+is\s+the\s+mom',
        r'^minutes\s+of\s+meeting[:\s]*\n',
        r'^i\s+have\s+(generated|created)',
        r'^based\s+on\s+(the\s+)?',
    ]
    
    for pattern in preambles:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)
    
    text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
    text = re.sub(r'\*([^*]+)\*', r'\1', text)
    text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*\+\s+', '  â€¢ ', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*-\s+([^S])', r'  â€¢ \1', text, flags=re.MULTILINE)
    text = re.sub(r'\n{4,}', '\n\n\n', text)
    
    if not text.strip().startswith('==='):
        text = '=' * 80 + '\n' + text
    
    return text.strip()

# =========================
# SUMMARIZE ENDPOINT
# =========================
@app.post("/summarize")
async def summarize(request: SummarizeRequest):
    if not model or not tokenizer:
        raise HTTPException(503, "Model not loaded")

    try:
        text_len = len(request.text)
        estimated_tokens = text_len // 4
        
        logger.info(f"[MOM] Processing ({text_len} chars, ~{estimated_tokens} tokens)")

        if estimated_tokens < 3500:
            logger.info("[MOM] Single-pass generation")
            
            user_prompt = f"""Generate comprehensive Minutes of Meeting.

CRITICAL:
- Start with ================================================================================
- NO preamble
- Use â€¢ for bullets
- Complete ALL sections
- Don't stop early

Transcription:

{request.text}"""

            analysis = generate_with_prompt(
                MEETING_ANALYSIS_SYSTEM_PROMPT,
                user_prompt,
                3000,
                request.temperature,
                is_synthesis=False
            )
            
            analysis = clean_mom_output(analysis)
            chunks_used = 1

        else:
            logger.info("[MOM] Chunking approach for long meeting")
            
            chunks = chunk_transcript_smartly(request.text)
            partial_analyses = []
            
            for i, chunk in enumerate(chunks):
                logger.info(f"[MOM] Analyzing chunk {i+1}/{len(chunks)}")
                
                chunk_prompt = f"""Analyze this meeting transcript section.

Extract:
- All speaker contributions
- All decisions with vote counts
- All action items
- Discussion details

Chunk {i+1}/{len(chunks)}:

{chunk}"""

                partial = generate_with_prompt(
                    MEETING_ANALYSIS_SYSTEM_PROMPT,
                    chunk_prompt,
                    MAX_NEW_TOKENS_PER_CHUNK,
                    request.temperature,
                    is_synthesis=False
                )
                
                partial_analyses.append(partial)
            
            logger.info("[MOM] Synthesizing...")
            
            combined = "\n\n=== CHUNK ===\n\n".join(partial_analyses)
            
            synthesis_prompt = f"""Synthesize into ONE complete MoM:

{combined}

Generate COMPLETE Minutes:"""

            analysis = generate_with_prompt(
                SYNTHESIS_SYSTEM_PROMPT,
                synthesis_prompt,
                MAX_NEW_TOKENS_SYNTHESIS,
                request.temperature,
                is_synthesis=True
            )
            
            analysis = clean_mom_output(analysis)
            chunks_used = len(chunks)
            
            if "Regards" not in analysis[-200:]:
                analysis += "\n\n" + "=" * 80 + "\n\nRegards,\nGenerated by AngelBot.AI\n"
        
        logger.info(f"[MOM] âœ“ Complete ({len(analysis)} chars)")

        return {
            "analysis": analysis,
            "original_length": text_len,
            "analysis_length": len(analysis),
            "chunks_used": chunks_used
        }

    except Exception as e:
        logger.error(f"[MOM] Failed: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(500, str(e))

# =========================
# TRANSLATION ENDPOINT (HIGH ACCURACY)
# =========================
@app.post("/translate")
async def translate(request: TranslateRequest):
    if not model or not tokenizer:
        raise HTTPException(503, "Model not loaded")

    try:
        source_lang = request.source_lang.lower()
        target_lang = request.target_lang.lower()
        
        # Validate
        if source_lang not in SUPPORTED_LANGUAGES:
            raise HTTPException(400, f"Unsupported source: {source_lang}")
        
        if target_lang not in SUPPORTED_LANGUAGES:
            raise HTTPException(400, f"Unsupported target: {target_lang}")
        
        if source_lang == target_lang:
            return {
                "translated_text": request.text,
                "source_lang": source_lang,
                "target_lang": target_lang,
                "method": "none",
                "note": "Same language"
            }
        
        source_info = SUPPORTED_LANGUAGES[source_lang]
        target_info = SUPPORTED_LANGUAGES[target_lang]
        
        logger.info(f"[TRANSLATE] {source_info['name']} â†’ {target_info['name']}")
        logger.info(f"[TRANSLATE] Input: {len(request.text)} chars")
        
        # Determine if pivot is needed
        use_pivot = request.use_pivot
        if use_pivot is None:
            # Auto-decide: use pivot for non-English pairs
            use_pivot = (source_lang != "en" and target_lang != "en")
        
        if use_pivot:
            logger.info(f"[TRANSLATE] Using PIVOT method: {source_lang} â†’ en â†’ {target_lang}")
        else:
            logger.info(f"[TRANSLATE] Using DIRECT method: {source_lang} â†’ {target_lang}")
        
        # Check if this is a diarized transcript (has timestamps/speakers)
        is_transcript = bool(re.search(r'\[\d{1,2}:\d{2}', request.text)) or 'SPEAKER_' in request.text
        
        if is_transcript:
            logger.info("[TRANSLATE] Detected transcript format - using line-by-line")
            translated = translate_transcript(
                request.text,
                source_lang,
                target_lang,
                request.temperature,
                use_pivot
            )
        else:
            # Plain text - translate directly
            if use_pivot:
                translated = translate_via_pivot(
                    request.text,
                    source_lang,
                    target_lang,
                    request.temperature
                )
            else:
                translated = translate_single_text(
                    request.text,
                    source_lang,
                    target_lang,
                    request.temperature
                )
        
        logger.info(f"[TRANSLATE] âœ“ Done ({len(translated)} chars)")

        return {
            "translated_text": translated,
            "source_lang": source_lang,
            "target_lang": target_lang,
            "source_language_name": source_info['name'],
            "target_language_name": target_info['name'],
            "target_direction": target_info['direction'],
            "method": "pivot" if use_pivot else "direct",
            "original_length": len(request.text),
            "translated_length": len(translated)
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[TRANSLATE] Failed: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(500, str(e))


# =========================
# SIMPLE TRANSLATION (FOR TESTING)
# =========================
class SimpleTranslateRequest(BaseModel):
    text: str
    source_lang: str
    target_lang: str

@app.post("/translate-simple")
async def translate_simple(request: SimpleTranslateRequest):
    """
    Simple translation endpoint for testing
    Always uses pivot for non-English pairs
    """
    if not model or not tokenizer:
        raise HTTPException(503, "Model not loaded")
    
    try:
        source_lang = request.source_lang.lower()
        target_lang = request.target_lang.lower()
        
        if source_lang not in SUPPORTED_LANGUAGES:
            raise HTTPException(400, f"Unsupported: {source_lang}")
        if target_lang not in SUPPORTED_LANGUAGES:
            raise HTTPException(400, f"Unsupported: {target_lang}")
        
        needs_pivot = source_lang != "en" and target_lang != "en"
        
        if needs_pivot:
            translated = translate_via_pivot(request.text, source_lang, target_lang, 0.1)
        else:
            translated = translate_single_text(request.text, source_lang, target_lang, 0.1)
        
        return {
            "original": request.text,
            "translated": translated,
            "source_lang": source_lang,
            "target_lang": target_lang,
            "method": "pivot" if needs_pivot else "direct"
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[SIMPLE] Failed: {e}")
        raise HTTPException(500, str(e))


# =========================
# VERIFY TRANSLATION ENDPOINT
# =========================
class VerifyTranslationRequest(BaseModel):
    original_text: str
    translated_text: str
    source_lang: str
    target_lang: str

@app.post("/verify-translation")
async def verify_translation(request: VerifyTranslationRequest):
    """
    Verify translation quality by back-translating
    """
    if not model or not tokenizer:
        raise HTTPException(503, "Model not loaded")
    
    try:
        # Back-translate
        back_translated = translate_single_text(
            request.translated_text,
            request.target_lang,
            request.source_lang,
            0.1
        )
        
        return {
            "original": request.original_text,
            "translated": request.translated_text,
            "back_translated": back_translated,
            "note": "Compare original with back_translated to assess quality"
        }
    
    except Exception as e:
        logger.error(f"[VERIFY] Failed: {e}")
        raise HTTPException(500, str(e))